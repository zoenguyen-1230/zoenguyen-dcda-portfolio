<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Experimentation | Zoe Nguyen</title>
    <link rel="stylesheet" href="css/styles.css">
</head>
<body>
    <header>
        <nav>
            <a href="index.html">Home</a>
            <a href="lab02.html" class="active">Lab 2: AI Experimentation</a>
            <!-- Add more lab links as the semester progresses -->
        </nav>
        <h1>AI Experimentation</h1>
    </header>

    <main>
        <section>
            <h2>What I Tried</h2>
            <p>For this lab, I set out to experiment with AI tools in ways that move beyond their typical productivity-oriented uses. Initially, my plan was to compare how two AI tools, specifically Claude and Leonardo.ai, would respond to the same unusual prompt: explaining the experience of eating phở on a sidewalk in Saigon, Vietnam, to a 60-year-old Midwestern American man who distrusts social media and dislikes travel. I was interested in how different tools translate a nuanced daily cultural experiences and what assumptions they make about “good” explanations or representations of culture.

            As I began the experiment, my approach shifted. Rather than directly comparing text-based outputs, I decided to intentionally use Leonardo.ai in a way it was clearly not designed for. Leonardo.ai is marketed primarily as an image-generation tool, not as a text interpreter. By prompting it to describe a specific cultural experience through image generation, I wanted to see how a visual AI system would handle a culturally specific everyday experience, exploring what assumptions or biases would surface when the tool was asked to perform something out of its intended function.</p>

             <figure class="viz-container">
                <img src="images/pho on sidewalk.jpg" alt="Pho on Sidewalk">
                <figcaption>The "Pho on the Sidewalk" visual I expected to get from AI tools.</figcaption>
            </figure>

            <figure class="viz-container">
                <img src="images/hanoi sidewalk.jpg" alt="Eating on a sidewalk in Hanoi">
                <figcaption>The "Pho on the Sidewalk" visual I expected to get from AI tools.</figcaption>
            </figure>
        </section>

        <section>
            <h2>What Happened</h2>
            <p>The results I got, however, were both surprising and unsettling. Instead of generating images that reflected the reality of sidewalk dining in Saigon with plastic stools, small tables, groups of people eating quickly or socially, the images repeatedly framed the experience as one of poverty or hardship. The images depicted people sitting directly on the sidewalk, often alone, wearing dirty or worn clothing, bended over for the food. The visual tone suggested a “humbling” or desperate experience rather than a common, convenient, and culturally common practice.

            This representation stood in sharp contrast to my lived experience. Sidewalk dining in Saigon is often social, efficient, and intentional. It is something people choose for convenience or preference, not something imposed by circumstance, and it does not preclude style, cleanliness, or enjoyment. The AI’s inability to capture this nuance revealed how narrowly it defines what “street food” looks like.

            The misrepresentation extended to the food itself. An image showed a bowl of red soup that did not resemble phở at all. In the same image output, Chinese characters appeared in signage or packaging, suggesting a blending of distinct cultural contexts. These details point to a lack of differentiation between Vietnamese cuisine and other East or Southeast Asian visual references within the generated imagery.

            While Claude’s text-based responses tended to romanticize the experience or over-explain it for a presumed Western reader, Leonardo.ai’s visual outputs revealed something different: deeply embedded assumptions about class, cleanliness, and non-Western urban life that were presented without commentary or hesitation.</p>

            <!-- Add screenshots or examples -->
            <figure class="viz-container">
                <img src="images/Pic 1.jpg" alt="Leonardo.ai Output 1">
                <figcaption>Leonardo.ai's Output to the prompt: "Explain the feeling of eating phở on a sidewalk in Saigon to a 60-year-old Midwestern American who distrusts social media and dislikes travel.</figcaption>
            </figure>

            <figure class="viz-container">
                <img src="images/Prompt 2.jpg" alt="Leonardo.ai Output 2">
                <figcaption>Leonardo.ai's Output to the prompt: "Explain the feeling of eating phở on a sidewalk in Saigon in a way that cannot be fully understood by someone who has never lived in Vietnam.</figcaption>
            </figure>

            <figure class="viz-container">
                <img src="images/Prompt 3.jpg" alt="Leonardo.ai Output 3">
                <figcaption>Leonardo.ai's Output to the prompt: "Explain the feeling of eating phở on a sidewalk in Saigon using only analytical or sociological language. Avoid emotional, sensory, or poetic descriptions.</figcaption>
            </figure>
        </section>

        <section>
            <h2>What I Learned</h2>
            <p>This experiment revealed how strongly AI tools rely on assumptions built into their training data and design. Leonardo.ai appears to associate sidewalk dining with poverty and deprivation, reflecting broader Western narratives about development and urban life in the Global South. The tool struggled to imagine sidewalk eating as something chosen, enjoyable, or culturally ordinary.

            I also learned that visual AI tools can be harder to interrogate than text-based models. While language models can sometimes acknowledge uncertainty or explain their reasoning, image generators simply produce outputs without explanation, making it more difficult to trace where biases originate, even when those biases are clearly visible in the images themselves.

            Several questions emerged that I could not fully answer. How much of this bias comes from the datasets used to train visual AI models? How much is shaped by the prompts and assumptions of prior users? And can visual AI ever represent everyday cultural practices without defaulting to stereotypes or symbolic shortcuts?</p>
        </section>

        <section>
            <h2>Looking Forward</h2>
            <p>This experiment has made me more critical and intentional about how I use AI tools in academic and professional contexts. While I still see value in AI for brainstorming, ideation, or technical support, I am more cautious about relying on it for cultural representation or visual storytelling. In future work, I would either use AI in limited, well-defined ways or deliberately analyze its failures, as I did here.

            Moving forward, I plan to ask deeper questions: What assumptions does this tool make about culture? Whose experiences are represented accurately, and whose are flattened or erased? Most importantly, where does the tool break down, and what do those breakdowns reveal about the systems and values embedded within it? Asking these questions will help me engage with AI tools more critically and intentionally.</p>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 Zoe Nguyen | DCDA 40833 | TCU | <a href="https://github.com/zoenguyen-1230/zoenguyen-dcda-portfolio">GitHub</a></p>
    </footer>
</body>
</html>